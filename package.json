{
  "name": "roollm-dev",
  "version": "1.0.0",
  "scripts": {
    "RooLLM": "cd api && python main.py",
    "interface": "cd frontend && python -m http.server 8080",
    "open:browser": "sleep 1 && echo \"Opening browser at http://localhost:8080\" && npx open-cli http://localhost:8080",
    "start": "npm run open:browser & concurrently --kill-others --handle-input -n RooLLM,interface -c green,cyan \"npm run RooLLM\" \"npm run interface\"",
    "RooLLM3": "cd api && python3 main.py",
    "interfac3": "cd frontend && python3 -m http.server 8080",
    "start3": "npm run open:browser & concurrently --kill-others --handle-input -n RooLLM,interface -c green,cyan \"npm run RooLLM3\" \"npm run interfac3\"",
    "prestart": "bash -c 'if command -v ollama >/dev/null 2>&1; then echo -e \"\\033[92mOllama is installed. You are using a local LLM API endpoint\\033[0m\"; else echo \"Ollama not installed. Install Ollama or set ROO_LLM_URL to a different endpoint.\"; exit 1; fi'",
    "prestart3": "bash -c 'if command -v ollama >/dev/null 2>&1; then echo -e \"\\033[92mOllama is installed. You are using a local LLM API endpoint\\033[0m\"; else echo \"Ollama not installed. Install Ollama or set ROO_LLM_URL to a different endpoint.\"; exit 1; fi'"
  },
  "devDependencies": {
    "concurrently": "^8.0.1",
    "open-cli": "^8.0.0"
  }
}